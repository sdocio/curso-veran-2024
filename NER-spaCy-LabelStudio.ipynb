{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91996fe7-7fc0-49d4-8b0b-1bc880a5dc17",
   "metadata": {},
   "source": [
    "# Anotación de entidades mencionadas: Trafilatura, spaCy e LabelStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c74f3d-e18d-4f6b-8699-85f17d245725",
   "metadata": {},
   "source": [
    "## Trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7e499-c63d-4510-9457-faf59bfc7dd8",
   "metadata": {},
   "source": [
    "Ferramenta deseñada para obter datos textuais a partir da web, e que se pode usar como programa desde a liña de comandos ou como módulo de Python.\n",
    "\n",
    "Instalación de Trafilatura usando `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfd9b0-d23c-49d0-9375-dc3039d4d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc10c92-f4ab-4a4f-9036-fe175a08f973",
   "metadata": {},
   "source": [
    "A opción `-h` proporciona axuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac77b6-c0aa-4bc3-920c-7848be9ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trafilatura -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9c7ee-d362-4311-9ecf-8826c23ef904",
   "metadata": {},
   "source": [
    "No ficheiro `list.txt` temos a lista das URLs que queremos descargar en formato texto para construír o noso corpus. Se a lista é grande, convén explorar as opcións de paralelización (`--parallel`). Trafilatura pode tamén actuar como un *web crawler*, obtendo sitios web completos ao seguir as ligazóns de forma recursiva (opcións de navegación: `--feed`, `--sitemap`, `--crawl`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649a1cf-a10d-43c2-9c8c-d176313c2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só desde Google Colab!\n",
    "!wget https://raw.githubusercontent.com/sdocio/curso-veran-2024/main/list.txt -O list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf4a1c-ff7a-41ad-806e-f4246a5ea446",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df71ef-8822-47fc-9f0c-5e3a34b2fccb",
   "metadata": {},
   "source": [
    "Iniciamos o programa indicándolle que preferimos texto limpo (`--precision`), non queremos comentarios (`--no-comments`) ou tabelas (`--no-tables`).\n",
    "\n",
    "A lista de URLs para extraer está no ficheiro `list.txt` (opción `-i`) e o resultado vai ser almacenado en `texts` (opción `-o`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d658241-14d1-4b1b-91b7-15ecedacff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trafilatura --no-comments --no-tables --precision -i list.txt -o texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca116be-c2e1-42f5-b75e-140435a591d7",
   "metadata": {},
   "source": [
    "Comprobamos que hai tres documentos limpos (sen etiquetas HTML) correspondentes ás entradas do ficheiro `list.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c2f68-c428-45dd-9385-bfab1e0d4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de07fd-c058-4153-9fa2-8ff2f5992541",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 texts/Di5K6oaTxVWyNSlB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2ec63-2e1b-4c9b-ac8d-6aa0c4f197c0",
   "metadata": {},
   "source": [
    "Máis información:\n",
    "\n",
    "- https://trafilatura.readthedocs.io/en/latest/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c377337-ace5-40f7-ac61-b21907f35222",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7585a9-39b0-4acc-99b9-e680a8e8e1ef",
   "metadata": {},
   "source": [
    "spaCy é un módulo ou biblioteca de Python para procesamento de linguaxe natural e soporte para máis de 64 linguas. Inclúe componentes como o recoñecemento de entidades mencionadas, etiquetación morfosintáctica, análise de dependencias sintácticas, segmentación de frases, clasificación de textos ou lematización, entre outros.\n",
    "\n",
    "![spaCy pipeline](https://spacy.io/images/pipeline.svg \"spaCy pipeline\")\n",
    "\n",
    "URL: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2338dd-5eb5-412a-9525-6939a1e34c5a",
   "metadata": {},
   "source": [
    "### Instalación de spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d54f90-9ee5-4b8b-a30e-d6395952edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44071a83-bb11-482a-ad61-3547ae72f135",
   "metadata": {},
   "source": [
    "### Instalación dun modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14516598-ee67-4651-8ea1-67c0c018ff98",
   "metadata": {},
   "source": [
    "Mediante `spacy download` descargamos un modelo de tamaño medio para inglés. Nun ambiente real de produción (e non de aprendizaxe ou mostra), a mellor opción sería descargar e usar un modelo maior, como `en_core_web_trf` (baseado en Transformers). Na cela, esta opción fica comentada para que sirva de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab51bef-1a62-4b52-885b-b3b57fa61d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf\n",
    "\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8c4bf-ab5f-438c-a78f-0a4c0e3e0033",
   "metadata": {},
   "source": [
    "### Carga do módulo e o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7202c-a950-48b5-8d46-acac575330f3",
   "metadata": {},
   "source": [
    "Cargamos o módulo (`import spacy`) e o modelo que queremos usar. Neste caso os textos están en inglés, polo que usaremos un modelo de tamaño medio (`md`) nesta lingua. Os modelos que apresentan un maior rendemento son os baseados en *Transformers* (`trf`), mais tamén son os que requiren máis recursos de cómputo e almacenamento.\n",
    "\n",
    "Máis modelos en: https://spacy.io/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87fbe4-04ab-4098-8593-db58c5b41788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370bb8b2-1b1e-4147-8fdc-345205610378",
   "metadata": {},
   "source": [
    "Lanzamos a *pipeline* de procesamento de spaCy chamando a `nlp()` e usando un pequeno texto de mostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9d157-9aae-4475-93f8-974560e7cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Google was founded by scientists Larry Page and Sergey Brin while they were students at Stanford University, in California.\"\n",
    "\n",
    "doc = nlp(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba67a8-2d7c-44cb-815d-027d065503c9",
   "metadata": {},
   "source": [
    "Unha vez lanzada  a *pipeline*, en `doc` temos os resultados de todos os seus componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39080102-66c8-47bb-8bf7-094158f5d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Componentes da pipeline\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c129d0-3c4b-4a13-8125-56a1d06b3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoS tagging (tagger)\n",
    "for token in doc:\n",
    "    print(f'{token.text} {token.lemma_} {token.pos_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee6ff0-6c38-4665-b0b0-1c2813073f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing (parser)\n",
    "for token in doc:\n",
    "    print(f'{token.text} {token.lemma_} {token.pos_} {token.dep_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e46a2-bf50-4761-b8fb-2b0c0f7abd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named-Entities Recognition and Classification (NERC)\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.text} {ent.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06200ced-d222-466f-87a0-61b42287b768",
   "metadata": {},
   "source": [
    "Algúns dos componentes (como o *parser* ou o *NERC*) inclúen un módulo de visualización.\n",
    "\n",
    "Exemplo de visualización do analizador sintáctico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb74a5-3e6b-4978-b8bf-e1aed05a5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "small_doc = nlp(\"Google was founded by Larry Page and Sergey Brin.\")\n",
    "displacy.render(small_doc, jupyter=True, style=\"dep\", options={\"distance\": 150})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9829ee-0584-4505-8a4e-35a8f72161a7",
   "metadata": {},
   "source": [
    "### Procesado do corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e984e-3560-4e5c-b2f5-c8c06077475c",
   "metadata": {},
   "source": [
    "Utilizaremos spaCy para procesar os textos do corpus e extraer as entidades mencionadas. A saída será un ficheiro JSON cunha estrutura que nos permita cargalo no editor Label Studio para unha corrección manual posterior dos erros do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335e4b9-adc8-4168-8b40-38ab1f07adee",
   "metadata": {},
   "source": [
    "Función que devolve un JSON estruturado para Label Studio a partir da *pipeline* de procesamento de spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee230a-ee94-465a-abb2-1df090432429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def doc_to_spans(doc):\n",
    "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
    "    results = []\n",
    "    entities = set()\n",
    "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
    "        if not entity:\n",
    "            continue\n",
    "        group = list(group)\n",
    "        _, start, _ = group[0]\n",
    "        word, last, _ = group[-1]\n",
    "        text = ' '.join(item[0] for item in group)\n",
    "        end = last + len(word)\n",
    "        results.append({\n",
    "            'from_name': 'label',\n",
    "            'to_name': 'text',\n",
    "            'type': 'labels',\n",
    "            'value': {\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text,\n",
    "                'labels': [entity]\n",
    "            }\n",
    "        })\n",
    "        entities.add(entity)\n",
    "\n",
    "    return results, entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1449d-466d-4a07-90d0-4e500c4069e9",
   "metadata": {},
   "source": [
    "Procesado dos textos e almacenamento dos ficheiros JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf34956-b2f7-4aec-840d-cf9229f6f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "# model = \"en_core_web_trf\"\n",
    "model = \"en_core_web_md\"\n",
    "\n",
    "nlp = spacy.load(model, disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "labels = set()\n",
    "\n",
    "for input_file in glob.glob('texts/*.txt'):\n",
    "    with open(input_file) as ifile:\n",
    "        output_file = input_file[:-3] + 'json'\n",
    "        with open(output_file, 'w') as ofile:\n",
    "            text = ifile.read()\n",
    "            predictions = []\n",
    "            tasks = []\n",
    "            doc = nlp(text)\n",
    "            results, ents = doc_to_spans(doc)\n",
    "            tasks.append({'data': {'text': text}, 'predictions': [{'result': results}]})\n",
    "            labels.update(ents)\n",
    "            json.dump(tasks, ofile, indent=2)\n",
    "\n",
    "print(\"Labels found:\")\n",
    "for label in labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00ae0e-b14c-4042-bc24-255f8e1809c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82324861-b7da-4518-ad52-53d682e293b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -20 texts/82KWPxwfRjzyVB4l.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c0cf1-73e3-4e29-b5bf-d589b5166ff8",
   "metadata": {},
   "source": [
    "Podemos usar displaCy para mostrar as entidades detectadas. Por exemplo, as do último documento procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74f1d6-d4d9-423a-a388-456676a558a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"colors\": {\"DATE\": \"lightgreen\"}}\n",
    "displacy.render(doc, jupyter=True, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd45a44-a219-42c5-a875-85aa215691c2",
   "metadata": {},
   "source": [
    "Para poder corrixir a anotación automática realizada por spaCy, descargamos os ficheiros JSON xerados para poder editalos a continuación en Label Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b13fb7-f48d-4150-9a35-a637f66ffd7e",
   "metadata": {},
   "source": [
    "## Label Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde837b4-f5ff-43ba-9884-6905593b5fb4",
   "metadata": {},
   "source": [
    "Label Studio é unha ferramenta flexíbel de anotación focada para a preparación de datos para adestrar modelos de aprendizaxe automática.\n",
    "\n",
    "URL: https://labelstud.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb5a83-eb1d-42f7-9788-41cfee93631d",
   "metadata": {},
   "source": [
    "### Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4cc11-e161-457f-b260-8d38d2597979",
   "metadata": {},
   "source": [
    "Instalación en local.\n",
    "\n",
    "1. Instalación de Miniconda (https://docs.anaconda.com/miniconda/miniconda-other-installer-links/)\n",
    "2. [Windows] Abrir Miniconda Powershell / [Linux/Mac] Abrir terminal\n",
    "4. Crear e activar ambiente virtual\n",
    "\n",
    "`conda create -n curso`  \n",
    "`conda activate curso`  \n",
    "`conda install psycopg2`\n",
    "\n",
    "4. Instalar Label Studio\n",
    "\n",
    "`pip install label-studio`\n",
    "\n",
    "5. Abrir o editor\n",
    "\n",
    "`label-studio`\n",
    "\n",
    "\n",
    "6. Crear unha conta (sign-up)\n",
    "7. Crear un proxecto\n",
    "\n",
    "- Nome do proxecto (*project name*)\n",
    "- Labelling setup: escoller Natural Language Processing, Named-Entity Recognition\n",
    "- Adicionar etiquetas  \n",
    "  No apartado de *labels*, eliminar as que hai e adicionar o listado que obtivemos no paso de xeración dos ficheiros JSON anotados.\n",
    "- Gardar o proxecto con *save*\n",
    "- Importar os documentos JSON xerados no apartado anterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
